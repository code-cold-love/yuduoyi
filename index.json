
[{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\nActor-Critic 既学习价值函数，又学习策略函数，它是囊括一系列算法的整体架构。Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。\nREINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。\nActor（策略网络）与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略； Critic（价值网络）要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断当前状态什么动作是好的、什么动作是不好的，从而帮助 Actor 进行策略更新。 Actor 的更新采用策略梯度的原则；将 Critic 价值网络表示为 \\(V_{\\omega}\\)，参数为 \\(\\omega\\)。于是，可以采用时序差分残差的学习方式，对单个数据定义如下价值函数的损失函数：\n$$\rL(\\omega) = \\frac{1}{2} (r + \\gamma V_{\\omega} (s_{t+1}) - V_{\\omega} (s_t))^2\r$$将上式中的 \\(r + \\gamma V_{\\omega} (s_{t+1})\\) 作为时序差分目标，不会产生梯度来更新价值函数。因此，价值函数的梯度为：\n$$\r\\nabla_{\\omega} L(\\omega) = -(r + \\gamma V_{\\omega}(s_{t+1}) - V_{\\omega}(s_t)) \\nabla_{\\omega} V_{\\omega} (s_t)\r$$然后使用梯度下降方法来更新 Critic 价值网络即可。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import torch import torch.nn.functional as F class PolicyNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): \u0026#34;\u0026#34;\u0026#34;策略网络\u0026#34;\u0026#34;\u0026#34; super(PolicyNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, action_dim) def forward(self, x: torch.Tensor): x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) class ValueNet(torch.nn.Module): def __init__(self, state_dim, hidden_dim): \u0026#34;\u0026#34;\u0026#34;价值网络\u0026#34;\u0026#34;\u0026#34; super(ValueNet, self).__init__() self.fc1 = torch.nn.Linear(state_dim, hidden_dim) self.fc2 = torch.nn.Linear(hidden_dim, 1) def forward(self, x: torch.Tensor): x = F.relu(self.fc1(x)) return self.fc2(x) class ActorCritic: def __init__(self, state_dim, hidden_dim, action_dim, actor_lr: float, critic_lr: float, gamma: float, device): self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.critic = ValueNet(state_dim, hidden_dim).to(device) self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) self.gamma = gamma self.device = device def take_action(self, state): state = torch.tensor([state], dtype=torch.float32).to(self.device) probs = self.actor(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): states = torch.tensor(transition_dict[\u0026#34;states\u0026#34;], dtype=torch.float32).to(self.device) actions = torch.tensor(transition_dict[\u0026#34;actions\u0026#34;]).view(-1, 1).to(self.device) rewards = torch.tensor(transition_dict[\u0026#34;rewards\u0026#34;], dtype=torch.float32).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[\u0026#34;next_states\u0026#34;], dtype=torch.float32).to(self.device) dones = torch.tensor(transition_dict[\u0026#34;dones\u0026#34;], dtype=torch.float32).view(-1, 1).to(self.device) # 时序差分目标 td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones) td_delta = td_target - self.critic(states) # 时序差分误差 log_probs = torch.log(self.actor(states).gather(1, actions)) actor_loss = torch.mean(-log_probs * td_delta.detach()) # 均方误差损失函数 critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach())) self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() actor_loss.backward() critic_loss.backward() self.actor_optimizer.step() self.critic_optimizer.step() ","date":"2025-07-03","externalUrl":null,"permalink":"/posts/actor-critic/","section":"Posts","summary":"","title":"Actor-Critic 算法","type":"posts"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/tags/hands-on-rl/","section":"Tags","summary":"","title":"Hands-on-Rl","type":"tags"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/series/hands-on-rl/","section":"Series","summary":"","title":"Hands-on-RL","type":"series"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/tags/rl/","section":"Tags","summary":"","title":"RL","type":"tags"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"2025-07-03","externalUrl":null,"permalink":"/","section":"鱼多矣","summary":"","title":"鱼多矣","type":"page"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n6.1 value-based 和 policy-based #\r基于价值的方法，如 Q-learning、DQN 及 DQN 改进算法，主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略。\n基于策略的方法则是显式地学习一个目标策略，如 REINFORCE 算法。\n6.2 策略梯度 #\r基于策略的方法首先需要将策略参数化：假设目标策略 \\(\\pi_{\\theta}\\) 是一个随机性策略，并且处处可微，其中 \\(\\theta\\) 是对应的参数。目标是寻找一个最优策略并最大化这个策略在环境中的期望回报（一个状态的期望回报被称为这个状态的价值）。可以将策略学习的目标函数定义为：\n$$\rJ(\\theta) = \\text{E}_{s_{0}} \\left[ V^{\\pi_{\\theta}}(s_{0}) \\right]\r$$其中，\\(s_0\\) 表示初始状态。有了目标函数后就可以对策略 \\(\\theta\\) 求导，得到导数后就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。\n$$\r\\nabla_{\\theta} J(\\theta) \\propto \\sum_{s\\in \\mathcal{S}}V^{\\pi_{\\theta}}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}}(s,a) \\nabla_{\\theta} \\pi_{\\theta} (a|s) \\\\\r= \\text{E}_{\\pi_{\\theta}} \\left[ Q^{\\pi_{\\theta}}(s,a) \\nabla_{\\theta} \\log \\pi_{\\theta} (a|s) \\right]\r$$因为上式中期望 \\(\\text{E}\\) 的下标是 \\(\\pi_{\\theta}\\)，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略 \\(\\pi_{\\theta}\\) 采样得到的数据来计算梯度。\n直观地理解策略梯度：梯度的修改是让策略更多地采样到带来较高 \\(Q\\) 值的动作，更少地采样到带来较低 \\(Q\\) 值的动作。\n6.3 REINFORCE #\r在计算策略梯度的公式中，需要用到 \\(Q^{\\pi_{\\theta}}(s,a)\\)，REINFORCE 算法采用蒙特卡洛方法来估计 \\(Q^{\\pi_{\\theta}}(s,a)\\)，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为：\n$$\r\\nabla_{\\theta} J(\\theta) = \\text{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\left( \\sum_{t'=t}^{T} \\gamma^{t'-t} r_t \\right) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t|s_t) \\right]\r$$其中，\\(T\\) 是和环境交互的最大步数。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np import torch import torch.nn as nn import torch.nn.functional as F class PolicyNet(nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super().__init__() self.fc1 = nn.Linear(state_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, action_dim) def forward(self, x) -\u0026gt; torch.Tensor: # 输入是某个状态, 输出则是该状态下的动作概率分布 x = F.relu(self.fc1(x)) return F.softmax(self.fc2(x), dim=1) class REINFORCE: def __init__(self, state_dim, hidden_dim, action_dim, lr: float, gamma: float, device): self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device) self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr) self.gamma = gamma # 折扣因子 self.device = device def tack_action(self, state): \u0026#34;\u0026#34;\u0026#34;根据动作概率分布随机采样\u0026#34;\u0026#34;\u0026#34; state = torch.tensor([state], dtype=torch.float).to(self.device) probs = self.policy_net(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict: dict): reward_list = transition_dict[\u0026#34;rewards\u0026#34;] state_list = transition_dict[\u0026#34;states\u0026#34;] action_list = transition_dict[\u0026#34;actions\u0026#34;] G = 0 self.optimizer.zero_grad() for i in reversed(range(len(reward_list))): reward = reward_list[i] state = torch.tensor([state_list[i]], dtype=torch.float).to(self.device) action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device) log_prob = torch.log(self.policy_net(state).gather(1, action)) G = self.gamma * G + reward loss = -log_prob * G loss.backward() self.optimizer.step() ","date":"2025-07-03","externalUrl":null,"permalink":"/posts/policy-gradient/","section":"Posts","summary":"","title":"策略梯度算法","type":"posts"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n5.1 model-based 和 model-free #\r根据是否具有环境模型（对环境的状态转移概率和奖励函数进行建模），强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）和无模型的强化学习（model-free reinforcement learning）。\nmodel-based RL 根据 Agent 与环境交互采样到的数据直接进行策略提升或者价值估计（如 Sarsa 和 Q-learning）； model-free RL 中，模型是可以事先知道的，也可以是根据 Agent 与环境交互采样到的数据学习得到的，然后用这个模型进行策略提升或者价值估计（如动态规划算法中的策略迭代和价值迭代、Dyna-Q）。 5.2 Dyna-Q 算法 #\rDyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。\nQ-planning 每次选取一个曾经访问过的状态 \\(s\\)，采取一个曾经在该状态下执行过的动作 \\(a\\)，通过模型得到转以后的状态 \\(s'\\) 以及奖励 \\(r\\)，并根据这个模拟数据 \\((s,a,r,s')\\) 用 Q-learning 的更新方式来更新动作价值函数。\nQ-learning 的时序差分更新方式为 $$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma \\max_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]$$ 在每次与环境交互执行一次 Q-learning 之后，Dyna-Q 会执行 \\(N\\) 次 Q-planning，其中 \\(N\\) 是一个可以事先选择的超参数，当其为 0 时就是普通的 Q-learning。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import random import numpy as np class DynaQ: def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_planning, n_action=4): \u0026#34;\u0026#34;\u0026#34;Dyna-Q 算法\u0026#34;\u0026#34;\u0026#34; self.Q_table = np.zeros((ncol * nrow, n_action)) # 初始化动作价值 Q(s, a) 表格 self.n_action = n_action # 动作个数 self.epsilon = epsilon self.alpha = alpha # 学习率 self.gamma = gamma # 折扣因子 self.n_planning = n_planning # 执行 1 次 Q-learning 后执行 Q-planning 的次数 self.model = dict() # 环境模型 def take_action(self, state): \u0026#34;\u0026#34;\u0026#34;选取下一步的操作\u0026#34;\u0026#34;\u0026#34; if np.random.random() \u0026lt; self.epsilon: action = np.random.randint(self.n_action) else: action = np.argmax(self.Q_table[state]) return action def q_learning(self, state, action, reward, next_state): \u0026#34;\u0026#34;\u0026#34;Q-learning\u0026#34;\u0026#34;\u0026#34; td_error = reward + self.gamma * self.Q_table[next_state].max() - self.Q_table[state, action] self.Q_table[state, action] += self.alpha * td_error def update(self, state, action, reward, next_state): self.q_learning(state, action, reward, next_state) self.model[(state, action)] = reward, next_state # 把数据添加到模型中 # Q-planning 循环 for _ in range(self.n_planning): # 随机选择曾经遇到过的状态动作对 (s, a), (r, s_) = random.choice(list(self.model.items())) self.q_learning(s, a, r, s_) 5.3 DQN 算法 #\r之前的 Q-learning 算法以一张表格来存储 \\(Q\\) 值，但这种做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用。当状态或者动作数量非常大，更甚者，当状态或者动作连续的时候，需要用函数拟合的方法来估计 \\(Q\\) 值：将这个复杂的 \\(Q\\) 值表格视作数据，使用一个参数化的函数 \\(Q_{\\theta}\\) 来拟合这些数据。\n由于神经网络具有强大的表达能力，因此可以用一个神经网络来表示函数 \\(Q\\)，称为 Q 网络。假设神经网络用来拟合 \\(Q\\) 的参数是 \\(\\omega\\)，即每一个状态 \\(s\\) 下所有可能动作 \\(a\\) 的 \\(Q\\) 值都能表示为 \\(Q_{\\omega}(s,a)\\)。\n对于一组数据 \\((s_i, a_i, r_i, s_i')\\)，将 Q 网络的损失函数构造为均方误差的形式：\n$$\r\\omega^{*} = \\arg \\min_{\\omega} \\frac{1}{2N} \\sum_{i=1}^{N} \\left[ Q_{\\omega}(s_i,a_i)-(r_i + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\omega}(s_i', a')) \\right]^2\r$$至此，就可以将 Q-learning 算法扩展到神经网络形式——深度 \\(Q\\) 网络（deep Q network, DQN）。\n经验回放 #\rDQN 是离线策略算法。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了经验回放方法，具体做法为维护一个回放缓冲区，将每次从环境中采样得到的 \\((s, a, r, s')\\) 存储到回放缓冲区中，训练 \\(Q\\) 网络时再从回放缓冲区中随机采样若干数据来进行训练。\n目标网络 #\rDQN 算法最终更新的目标是让 \\(Q_{\\omega}(s,a)\\) 逼近 \\(r+\\gamma \\max_{a'}Q_{\\omega}(s',a')\\)，由于 TD 误差目标本身就包含神经网络的输出，因此在更新网络参数的同时目标也在不断地改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN 使用了目标网络的思想：\n原来的训练网络 \\(Q_{\\omega}(s,a)\\)，用于计算原来的损失函数 \\(\\frac{1}{2} \\left[ Q_{\\omega}(s,a)-(r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\omega^{-}}(s', a')) \\right]^2\\) 中的 \\(Q_{\\omega}(s,a)\\) 项，并使用正常梯度下降方法来进行更新。 目标网络 \\(Q_{\\omega^{-}}\\)，用于计算原来的损失函数 \\(\\frac{1}{2} \\left[ Q_{\\omega}(s,a)-(r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\omega^{-}}(s', a')) \\right]^2\\) 中的 \\(r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\omega^{-}} (s',a')\\) 项，其中 \\(\\omega^{-}\\) 为目标网络中的参数。 为了让更新目标更稳定，目标网络使用训练网络的一套比较旧的参数，训练网络 \\(Q_{\\omega}(s,a)\\) 在训练中的每一步都会更新，而目标网络的参数每隔 \\(C\\) 步才会与训练网络同步一次，即 \\(\\omega^{-} \\leftarrow \\omega\\)。\n5.4 Double DQN 算法 #\r普通的 DQN 算法通常会导致对 \\(Q\\) 值的过高估计：考虑到通过神经网络估算的 \\(Q\\) 值本身在某些时候会产生正向或负向的误差，在 DQN 的更新方式下神经网络会将正向误差累积。\n为了解决这一问题，Double DQN 算法提出利用两套独立训练的神经网络来估算 \\(\\max_{a'} Q^{*}(s', a')\\)，具体做法是将原来的 \\(\\max_{a'} Q_{\\omega^{-}} (s',a')\\) 更改为 \\(Q_{\\omega^{-}}(s', \\arg \\max_{a'} Q_{\\omega}(s',a'))\\)。\nDQN 与 Double DQN 的差别只在于计算状态 \\(s'\\) 下的 Q 值时如何选取动作：\nDQN 的优化目标可以写为 \\(r + \\gamma Q_{\\omega^{-}} (s', \\arg \\max_{a'} Q_{\\omega^{-}} (s',a'))\\)，动作的选取依靠目标网络 \\(Q_{\\omega^{-}}\\)； Double DQN 的优化目标为 \\(r + \\gamma Q_{\\omega^{-}} (s', \\arg \\max_{a'} Q_{\\omega} (s',a'))\\)，动作的选取依靠训练网络 \\(Q_{\\omega}\\)。 5.5 Dueling DQN 算法 #\rDueling DQN 是 DQN 的另一种改进算法。在强化学习中，将动作价值函数 \\(Q\\) 减去状态价值函数 \\(V\\) 的结果定义为优势函数 \\(A\\)，即 \\(A(s,a) = Q(s,a) - V(s)\\)。在同一个状态下，所有动作的优势值之和为 0，因为所有动作的动作价值的期望就是这个状态的状态价值。据此，在 Dueling DQN 中，\\(Q\\) 网络被建模为：\n$$\rQ_{\\eta, \\alpha, \\beta} (s,a) = V_{\\eta, \\alpha}(s) + A_{\\eta, \\beta}(s,a)\r$$其中，\\(V_{\\eta, \\alpha}(s)\\) 为状态价值函数，而 \\(A_{\\eta, \\beta}(s,a)\\) 则为该状态下采取不同动作的优势函数，表示不同动作的差异性：\\(\\eta\\) 是 \\(V\\) 和 \\(A\\) 共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而 \\(\\alpha\\) 和 \\(\\beta\\) 分别为 \\(V\\) 和 \\(A\\) 的参数。\n在这样的模型下，不再让神经网络直接输出 \\(Q\\) 值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到 \\(Q\\) 值。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import random from collections import deque import numpy as np import torch import torch.nn as nn import torch.nn.functional as F class ReplayBuffer: def __init__(self, capacity): \u0026#34;\u0026#34;\u0026#34;经验回放池\u0026#34;\u0026#34;\u0026#34; self.buffer = deque(maxlen=capacity) def add(self, state, action, reward, next_state, done): \u0026#34;\u0026#34;\u0026#34;加入数据\u0026#34;\u0026#34;\u0026#34; self.buffer.append((state, action, reward, next_state, done)) def sample(self, batch_size): \u0026#34;\u0026#34;\u0026#34;采样数据\u0026#34;\u0026#34;\u0026#34; transitions = random.sample(self.buffer, batch_size) state, action, reward, next_state, done = zip(*transitions) return np.array(state), action, reward, np.array(next_state), done def size(self): \u0026#34;\u0026#34;\u0026#34;返回当前回放池的大小\u0026#34;\u0026#34;\u0026#34; return len(self.buffer) class Qnet(nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): \u0026#34;\u0026#34;\u0026#34;只有一层隐藏层的 Q 网络\u0026#34;\u0026#34;\u0026#34; super().__init__() self.fc1 = nn.Linear(state_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, action_dim) def forward(self, x) -\u0026gt; torch.Tensor: x = F.relu(self.fc1(x)) return self.fc2(x) class VAnet(nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): \u0026#34;\u0026#34;\u0026#34;只有一层隐藏层的优势函数 A 网络和状态价值函数 V 网络\u0026#34;\u0026#34;\u0026#34; super().__init__() self.fc1 = nn.Linear(state_dim, hidden_dim) # 共享网络部分 self.fc_A = nn.Linear(hidden_dim, action_dim) self.fc_V = nn.Linear(hidden_dim, 1) def forward(self, x: torch.Tensor): A = self.fc_A(F.relu(self.fc1(x))) V = self.fc_V(F.relu(self.fc1(x))) Q = V + A - A.mean(1).view(-1, 1) # Q 值由 V 值和 A 值计算得到 return Q class DQN: def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device, dqn_type=\u0026#34;VanillaDQN\u0026#34;): \u0026#34;\u0026#34;\u0026#34;DQN 算法,包括 Double DQN 和 Dueling DQN\u0026#34;\u0026#34;\u0026#34; self.action_dim = action_dim if dqn_type == \u0026#34;DuelingDQN\u0026#34;: # Dueling DQN 采取不一样的网络框架 self.q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = VAnet(state_dim, hidden_dim, self.action_dim).to(device) else: self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate) self.gamma = gamma self.epsilon = epsilon self.target_update = target_update self.count = 0 self.dqn_type = dqn_type self.device = device def take_action(self, state): if np.random.random() \u0026lt; self.epsilon: action = np.random.randint(self.action_dim) else: state = torch.tensor([state], dtype=torch.float).to(self.device) action = self.q_net(state).argmax().item() return action def max_q_value(self, state): state = torch.tensor([state], dtype=torch.float).to(self.device) return self.q_net(state).max().item() def update(self, transition_dict): states = torch.tensor(transition_dict[\u0026#34;states\u0026#34;], dtype=torch.float).to(self.device) actions = torch.tensor(transition_dict[\u0026#34;actions\u0026#34;]).view(-1, 1).to(self.device) rewards = torch.tensor(transition_dict[\u0026#34;rewards\u0026#34;], dtype=torch.float).view(-1, 1).to(self.device) next_states = torch.tensor(transition_dict[\u0026#34;next_states\u0026#34;], dtype=torch.float).to(self.device) dones = torch.tensor(transition_dict[\u0026#34;dones\u0026#34;], dtype=torch.float).view(-1, 1).to(self.device) q_values = self.q_net(states).gather(1, actions) if self.dqn_type == \u0026#34;DoubleDQN\u0026#34;: max_action = self.q_net(next_states).max(1)[1].view(-1, 1) max_next_q_values = self.target_q_net(next_states).gather(1, max_action) else: max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1) q_targets = rewards + self.gamma * max_next_q_values * (1 - dones) dqn_loss = torch.mean(F.mse_loss(q_values, q_targets)) self.optimizer.zero_grad() dqn_loss.backward() self.optimizer.step() if self.count % self.target_update == 0: self.target_q_net.load_state_dict(self.q_net.state_dict()) self.count += 1 ","date":"2025-07-02","externalUrl":null,"permalink":"/posts/deep-q-network/","section":"Posts","summary":"","title":"Deep Q Network 算法","type":"posts"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n无模型（model-free）的强化学习，如 Sarsa 和 Q-learning，智能体只能和环境进行交互，通过采样到的数据来学习。不同于动态规划算法，model-free 的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。\n将采样数据的策略称为行为策略（behavior policy），称用这些数据来更新的策略为目标策略（target policy）。\n在线策略（on-policy）学习表示行为策略和目标策略是同一个策略，要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了。\n离线策略（off-policy）学习表示行为策略和目标策略不是同一个策略，使用经验回放池将之前采样得到的样本收集起来再次利用，能更好地利用历史数据，并具有更小的样本复杂度。\n4.1 时序差分 #\r时序差分（temporal difference, TD）结合了蒙特卡洛和动态规划算法的思想：\nTD 和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境； TD 和动态规划的相似之处在于可以根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。 4.2 Sarsa #\rSarsa 的更新公式必须使用当前策略采样得到的五元组 \\((s,a,r,s',a')\\)，因此它是 on-policy 算法。它直接使用 TD 算法来估计动作价值函数 \\(Q(s,a)\\)：\n$$\rQ(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t) \\right]\r$$然后用 \\(\\epsilon-greedy\\) 算法根据动作价值选取动作来和环境交互，再根据得到的数据用 TD 算法更新动作价值估计。\n$$\r\\pi(a|s) = \\begin{cases}\r\\epsilon/|A|+1-\\epsilon \u0026 \\text{if } a = \\arg \\max_{a'} Q(s,a') \\\\\r\\epsilon/|A| \u0026 \\text{otherwise}\r\\end{cases}\r$$#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np class Sarsa: def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4): self.Q_table = np.zeros([nrow * ncol, n_action]) # 初始化Q(s,a)表格 self.n_action = n_action # 动作个数 self.alpha = alpha # 学习率 self.gamma = gamma # 折扣因子 self.epsilon = epsilon # epsilon-贪婪策略中的参数 def take_action(self, state) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;选取下一步的操作,具体实现为 epsilon-贪婪\u0026#34;\u0026#34;\u0026#34; if np.random.random() \u0026lt; self.epsilon: action = np.random.randint(self.n_action) else: action = np.argmax(self.Q_table[state]) return action def best_action(self, state) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34;打印状态 s 对应的策略\u0026#34;\u0026#34;\u0026#34; Q_max = np.max(self.Q_table[state]) a = [0 for _ in range(self.n_action)] for i in range(self.n_action): # 若两个动作的价值一样,都会记录下来 if self.Q_table[state, i] == Q_max: a[i] = 1 return a def update(self, s0, a0, r, s1, a1): td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0] self.Q_table[s0, a0] += self.alpha * td_error 4.3 多步Sarsa #\r蒙特卡洛方法无偏的，但是具有比较大的方法；TD 算法具有非常小的方差，但它是有偏的。多步时序差分结合二者的优势，使用 \\(n\\) 步的奖励，然后使用之后状态的价值估计，用公式表示：将 \\(G_t=r_t+\\gamma Q(s_{t+1},a_{t+1})\\) 替换成 \\(G_t = r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{n}Q(s_{t+n},a_{t+n})\\)。\n于是，相应存在一种多步 Sarsa 算法，它的动作价值函数更新公式变为：\n$$\rQ(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{n} Q(s_{t+n},a_{t+n}) - Q(s_t,a_t) \\right]\r$$#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np class nstep_Sarsa: def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4): self.Q_table = np.zeros([nrow * ncol, n_action]) self.n_action = n_action self.alpha = alpha self.gamma = gamma self.epsilon = epsilon self.n = n # 采用 n 步 Sarsa 算法 self.state_list = [] # 保存之前的状态 self.action_list = [] # 保存之前的动作 self.reward_list = [] # 保存之前的奖励 def take_action(self, state) -\u0026gt; int: if np.random.random() \u0026lt; self.epsilon: action = np.random.randint(self.n_action) else: action = np.argmax(self.Q_table[state]) return action def best_action(self, state) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34;打印状态 s 对应的策略\u0026#34;\u0026#34;\u0026#34; Q_max = np.max(self.Q_table[state]) a = [0 for _ in range(self.n_action)] for i in range(self.n_action): if self.Q_table[state, i] == Q_max: a[i] = 1 return a def update(self, s0, a0, r, s1, a1, done): # 保存之前的状态、动作和奖励 self.state_list.append(s0) self.action_list.append(a0) self.reward_list.append(r) if len(self.state_list) == self.n: # 若保存的数据可以进行 n 步更新 G = self.Q_table[s1, a1] # 对应 Q(s_{t+n}, a_{t+n}) for i in reversed(range(self.n)): G = self.gamma * G + self.reward_list[i] # 不断向前计算每一步的回报 if done and i \u0026gt; 0: # 如果到达终止状态,最后几步虽然长度不够 n 步,也将其进行更新 s = self.state_list[i] a = self.action_list[i] self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a]) # 将需要更新的状态动作从列表中删除,下次不必更新 s = self.state_list.pop(0) a = self.action_list.pop(0) self.reward_list.pop(0) # n 步 Sarsa 的主要更新步骤 self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a]) if done: # 如果到达终止状态,即将开始下一条序列,则将列表全清空 self.state_list = [] self.action_list = [] self.reward_list = [] 4.4 Q-learning #\rQ-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为\n$$\rQ(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_t + \\gamma \\max_{a} Q(s_{t+1},a) - Q(s_t,a_t) \\right]\r$$Q-learning 的更新公式使用四元组 \\((s,a,r,s')\\) 来更新当前状态动作对的动作价值 \\(Q(s,a)\\)，数据中的 \\(s\\) 和 \\(a\\) 是给定的条件，\\(r\\) 和 \\(s'\\) 皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是 off-policy 算法。\n需要强调的是，Q-learning 的更新并非必须使用当前贪婪策略 \\(\\arg \\max_a Q(s,a)\\) 采样得到的数据，因为给定任意 \\((s,a,r,s')\\) 都可以直接根据更新公式来更新 \\(Q\\)。为了探索，通常使用一个 \\( \\epsilon-greedy \\) 策略来与环境交互；而 Sarsa 必须使用当前 \\( \\epsilon-greedy \\) 策略采样得到数据。\n下列代码中以矩阵的方式建立了一张存储每个状态下所有动作的 \\(Q\\) 值的表格。表格中的每一个动作价值 \\(Q(s,a)\\) 表示在状态 \\(s\\) 下选择动作 \\(a\\) 然后继续遵循某一策略预期得到的期望回报。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np class QLearning: def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4): self.Q_table = np.zeros([nrow * ncol, n_action]) # 初始化 Q(s,a) 表格 self.n_action = n_action # 动作个数 self.alpha = alpha # 学习率 self.gamma = gamma # 折扣因子 self.epsilon = epsilon # epsilon-贪婪策略中的参数 def take_action(self, state): \u0026#34;\u0026#34;\u0026#34;选取下一步的操作\u0026#34;\u0026#34;\u0026#34; if np.random.random() \u0026lt; self.epsilon: action = np.random.randint(self.n_action) else: action = np.argmax(self.Q_table[state]) return action def best_action(self, state): Q_max = np.max(self.Q_table[state]) a = [0 for _ in range(self.n_action)] for i in range(self.n_action): if self.Q_table[state, i] == Q_max: a[i] = 1 return a def update(self, s0, a0, r, s1): td_error = r + self.gamma * self.Q_table[s1].max() - self.Q_table[s0, a0] self.Q_table[s0, a0] += self.alpha * td_error ","date":"2025-06-22","externalUrl":null,"permalink":"/posts/temporal-difference/","section":"Posts","summary":"","title":"时序差分算法","type":"posts"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n基于动态规划的两种强化学习算法（策略迭代和价值迭代）要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔科夫决策过程。但是，现实中的白盒环境很少，无法将其运用在很多实际场景中。另外，策略迭代和价值迭代通常只适用于有限马尔科夫决策过程，即状态空间和动作空间是离散且有限的。\n公式回顾： $$\r\\begin{equation*}\r\\begin{aligned}\r\u0026 V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) Q^{\\pi}(s,a) \\\\\r\u0026 Q^{\\pi}(s,a)= r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V^{\\pi} (s') \\\\\r\\\\\r\u0026 V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s,a)+\\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V^{\\pi}(s') \\right) \\\\\r\u0026 Q^{\\pi} (s,a) = r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') Q^{\\pi}(s', a') \\\\\r\\\\\r\u0026 V^{*}(s)=\\max_{a \\in \\mathcal{A}} \\{r(s,a)+\\gamma \\sum_{s' \\in \\mathcal{S}}P(s'|s,a)V^{*}(s') \\} \\\\\r\u0026 Q^{*}(s,a)=r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\max_{a' \\in \\mathcal{A}} Q^{*}(s',a')\r\\end{aligned}\r\\end{equation*}\r$$ 3.1 策略迭代算法 #\r策略迭代（policy iteration）是策略评估（policy evaluation）和策略提升（policy improvement）不断循环交替，直至最后得到最优策略 \\(\\pi^{*}\\) 的过程。\n$$\r\\pi^{0} \\rightarrow V^{\\pi^{0}} \\rightarrow \\pi^{1} \\rightarrow V^{\\pi^{1}} \\rightarrow \\pi^{2} \\rightarrow V^{\\pi^{2}} \\rightarrow \\cdots \\rightarrow \\pi^{*}\r$$\r3.1.1 策略评估 #\r策略评估这一过程使用贝尔曼期望方程来计算一个策略的状态价值函数 \\(V^{\\pi}(s)\\)。考虑所有的状态，用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即 $$\rV^{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) V^{k}(s') \\right)\r$$可以选定任意初始值 \\(V^{0}\\)，根据贝尔曼期望方程，可以得知 \\(V^{k}=V^{\\pi}\\) 是以上更新公式的一个不动点。在实际的实现过程中，如果某一轮 \\( \\max_{s \\in \\mathcal{S}} |V^{k+1}(s) - V^{k}(s)| \\) 的值非常小，可以提前结束策略评估。\n3.1.2 策略提升 #\r策略提升定理：假设存在一个确定性策略 \\( \\pi' \\)，在任意一个状态 \\(s\\) 下，都满足 \\( Q^{\\pi}(s, \\pi'(s)) \\geq V^{\\pi}(s) \\)，于是在任意状态 \\(s\\) 下，有 \\(V^{\\pi'}(s) \\geq V^{\\pi}(s)\\)。\n于是可以直接贪心地在每一个状态选择动作价值最大的动作，也就是 $$\r\\pi'(s) = \\arg \\max_a Q^{\\pi}(s, a) = \\arg \\max_a \\{r(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi} (s')\\}\r$$我们发现构造的贪心策略 \\(\\pi'\\) 满足策略提升定理的条件，所以 \\(\\pi'\\) 比 \\(\\pi\\) 更好或者至少与其一样好。当策略提升之后的策略和之前的策略一样时（\\(\\pi'=\\pi\\)），说明策略迭代达到了收敛，此时 \\(\\pi\\) 和 \\(\\pi'\\) 就是最优策略。\n3.2 价值迭代算法 #\r价值迭代（value iteration）过程中不存在显式的策略，只维护一个状态价值函数 \\(V(s)\\)。价值迭代算法利用的是贝尔曼最优方程，将其写成迭代更新的方式： $$\rV^{k+1}(s) = \\max_{a \\in \\mathcal{A}} \\{ r(s,a)+\\gamma \\sum_{s' \\in \\mathcal{S}}P(s'|s,a)V^{k}(s') \\}\r$$等到 \\(V^{k+1}\\) 和 \\(V^{k}\\) 相同时，它就是贝尔曼最优方程的不动点，此时对应着最优状态价值函数 \\(V^{*}(s)\\)。然后利用 \\(\\pi(s)=\\arg \\max_{a} \\{r(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^{k+1}(s')\\}\\) 从中恢复出最优策略即可。\n价值迭代中的循环次数远少于策略迭代。\n","date":"2025-06-21","externalUrl":null,"permalink":"/posts/dynamic-programming/","section":"Posts","summary":"","title":"动态规划算法","type":"posts"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n2.1 状态和马尔科夫过程 #\r\\(S_{t}\\) 表示随机现象在某时刻 \\(t\\) 的取值，是一个向量随机变量，所有可能的状态组成状态集合 \\(\\mathcal{S}\\)。 马尔科夫性质：当且仅当某时刻的状态只取决于上一时刻的状态，用公式表示为 \\(P(S_{t+1} | S_t) = P(S_{t+1} | S_1, \\cdots, S_t)\\)。\n虽然 \\(t+1\\) 时刻的状态只与 \\(t\\) 时刻的状态有关，但是 \\(t\\) 时刻的状态其实包含了 \\(t-1\\) 时刻的状态信息，通过这种链式的关系，历史的信息被传递到了现在。\n马尔科夫过程是指具有马尔科夫性质的随机过程，也被称为马尔科夫链。通常用元组 \\(\u003c\\mathcal{S}, \\mathcal{P}\u003e\\) 描述一个马尔科夫过程，其中 \\(\\mathcal{S}\\) 是有限数量的状态集合，\\(\\mathcal{P}\\) 是状态转移矩阵，它定义了所有状态对之间的转移概率。\n$$\r\\mathcal{P} =\r\\left[ \\begin{matrix}\r\u0026P(s_1 | s_1) \u0026\\cdots \u0026P(s_n | s_1) \\\\\r\u0026\\vdots \u0026\\ddots \u0026\\vdots \\\\\r\u0026P(s_1 | s_n) \u0026\\cdots \u0026P(s_n | s_n)\r\\end{matrix} \\right]\r$$终止状态不会再转移到其他状态，永远以概率 1 转移到自己。\n2.2 马尔科夫奖励过程 MRP #\r在马尔科夫过程的基础上加入奖励函数 \\(r\\) 和折扣因子 \\(\\gamma\\)，就可以得到 MRP，即一个 MRP 由 \\(\u003c\\mathcal{S}, \\mathcal{P}, r, \\gamma\u003e\\) 构成。\n\\(r\\) 是奖励函数，某个状态 \\(s\\) 的奖励 \\(r(s)\\) 指转移到该状态时可以获得的奖励的期望。 \\(0 \\leq \\gamma \u003c 1\\) 是折扣因子，引入折扣因子是因为远期利益具有一定的不确定性，有时更希望能够尽快获得一些奖励，所以需要对远期利益打一些折扣。\\(\\gamma\\) 接近 0 更考虑短期奖励，\\(\\gamma\\) 接近 1 更关注长期奖励。 回报：令 \\(R_t\\) 表示在时刻 \\(t\\) 获得的奖励，在一个 MRP 中，从 \\(t\\) 时刻状态 \\(S_t\\) 开始直到终止状态时，所有奖励的衰减之和称为回报，即 \\(G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k}\\)。\n价值函数：在 MRP 中，一个状态的期望回报被称为这个状态的价值，所有状态的价值就组成了价值函数。价值函数的输入为某个状态，输出为这个状态的价值，将其写成 \\(V(s) = \\text{E}[G_t | S_t = s]\\)。\n$$\r\\begin{equation*}\r\\begin{aligned}\rV(s) \u0026= \\text{E} [G_t | S_t = s] = \\text{E} [R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\cdots | S_t = s] \\\\\r\u0026=\\text{E} [R_t + \\gamma G_{t+1} | S_t = s] \\\\\r\u0026=\\text{E} [R_t + \\gamma V(S_{t+1}) | S_t=s]\r\\end{aligned}\r\\end{equation*}\r$$\r2.3 马尔科夫决策过程 MDP #\r在 MRP 的基础上加入外界智能体的动作，就得到了马尔科夫决策过程。一个 MDP 由元组 \\(\u003c\\mathcal{S}, \\mathcal{A}, P, r, \\gamma\u003e\\) 构成。\n\\(\\mathcal{A}\\) 是动作的集合。 \\(r(s, a)\\) 可以同时取决于状态和动作，在只取决于状态时，则退化为 \\(r(s)\\)。 \\(P(s' | s,a)\\) 是状态转移函数，表示在状态 \\(s\\) 下执行动作 \\(a\\) 之后到达状态 \\(s'\\) 的概率。 注意，这里不再使用类似 MRP 定义中的状态转移矩阵，而是直接使用状态转移函数。 智能体根据当前状态从 \\(\\mathcal{A}\\) 中选择一个动作的函数，被称为策略，通常用 \\(\\pi\\) 表示。策略 \\(\\pi(a | s) = P(A_t = a | S_t = s)\\) 表示在输入状态 \\(s\\) 的情况下采取动作 \\(a\\) 的概率。\n在 MDP 中，由于马尔可夫性质的存在，策略 \\(\\pi\\) 只需要与当前状态有关，不需要考虑历史状态。\n动作价值函数 \\(Q^{\\pi} (s, a)\\) 表示在 MDP 遵循策略 \\(\\pi\\) 时，对当前状态 \\(s\\) 执行动作 \\(a\\) 得到的期望回报：\n$$\rQ^{\\pi}(s,a)=\\text{E}_{\\pi} [G_t | S_t=s, A_t=a] = r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V^{\\pi} (s')\r$$\r小结与贝尔曼方程 #\r\\(\\mathcal{P}\\) 状态转移矩阵，定义了所有状态对之间的转移概率；\\(P(s'|s,a)\\) 状态转移函数，表示在状态 \\(s\\) 下执行动作 \\(a\\) 之后到达状态 \\(s'\\) 的概率。状态转移函数与动作有关，并且状态转移函数更具有一般意义。 \\(\\mathcal{A}\\) 表示 Agent 动作的集合。 \\(R_t\\) 表示在时刻 \\(t\\) 获得的奖励。 \\(r(s)\\) 奖励函数，指转移到该状态时可以获得的奖励的期望，\\(r(s) = \\text{E} [R_t | S_t = s]\\)；\\(r(s, a)\\) 可以同时取决于状态和动作。 \\(G_t\\) 回报，指从 \\(t\\) 时刻状态 \\(S_t\\) 到终止状态时，所有奖励的衰减之和，\\(G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k}\\)。 \\(V(s)\\) 状态价值函数，指一个状态的期望回报，\\(V(s) = \\text{E}[G_t | S_t = s]\\)。 策略 \\(\\pi(a | s) = P(A_t = a | S_t = s)\\) 表示在输入状态 \\(s\\) 的情况下采取动作 \\(a\\) 的概率 \\(Q^{\\pi} (s, a)\\) 动作价值函数，表示在 MDP 遵循策略 \\(\\pi\\) 时，对当前状态 \\(s\\) 执行动作 \\(a\\) 得到的期望回报。 状态价值函数和动作价值函数之间的关系： $$\r\\begin{equation*}\r\\begin{aligned}\r\u0026 V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a | s) Q^{\\pi}(s,a) \\\\\r\u0026 Q^{\\pi}(s,a)= r(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V^{\\pi} (s')\r\\end{aligned}\r\\end{equation*}\r$$\r贝尔曼方程 #\r马尔科夫奖励过程 MRP 中： $$\rV(s) = r(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' | s) V(s')\r$$若一个 MRP 一共有 \\(n\\) 个状态，即 \\(\\mathcal{S} = \\{s_1, s_2, \\cdots, s_n\\}\\)，将所有状态的价值表示成一个列向量 \\(\\mathcal{V} = [V(s_1), V(s_2), \\cdots, V(s_n)]^T\\)；同理，将奖励函数表示成一个列向量 \\(\\mathcal{R} = [r(s_1), r(s_2), \\cdots, r(s_n)]^T\\)。于是可以将贝尔曼方程写成矩阵的形式： $$\r\\mathcal{V}=\\mathcal{R}+\\gamma \\mathcal{P} \\mathcal{V} \\Rightarrow \\mathcal{V}=(\\mathcal{I}-\\gamma \\mathcal{P})^{-1} \\mathcal{R}\r$$上述解析解的计算复杂度是 \\(O(n^3)\\)，其中 \\(n\\) 是状态个数，因此这种方法只适用于很小的马尔可夫奖励过程。\n贝尔曼期望方程 #\r马尔科夫决策过程 MDP 中： $$\r\\begin{equation*}\r\\begin{aligned}\r\u0026 V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) \\left( r(s,a)+\\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) V^{\\pi}(s') \\right) \\\\\r\u0026 Q^{\\pi} (s,a) = r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' | s') Q^{\\pi}(s', a')\r\\end{aligned}\r\\end{equation*}\r$$\r贝尔曼最优方程 #\r最优策略 \\(\\pi^{*}(s)\\) 下由相同的最优状态价值函数和最优动作价值函数： $$\r\\begin{equation*}\r\\begin{aligned}\r\u0026 V^{*}(s)=\\max_{a \\in \\mathcal{A}} \\{r(s,a)+\\gamma \\sum_{s' \\in \\mathcal{S}}P(s'|s,a)V^{*}(s') \\} \\\\\r\u0026 Q^{*}(s,a)=r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s,a) \\max_{a' \\in \\mathcal{A}} Q^{*}(s',a')\r\\end{aligned}\r\\end{equation*}\r$$","date":"2025-06-20","externalUrl":null,"permalink":"/posts/markov-decision-process/","section":"Posts","summary":"","title":"马尔科夫决策过程","type":"posts"},{"content":" 本系列是学习《动手学强化学习》\r过程中做的摘抄。\n1.1 问题介绍 #\r多臂老虎机（multi-armed bandit, MAB）问题中，有一个拥有 \\(K\\) 根拉杆的老虎机，拉动每一根拉杆都对应一个奖励的概率分布 \\(\\mathcal{R}\\)。 多臂老虎机问题可以表示为一个元组 \\(\u003c\\mathcal{A}, \\mathcal{R}\u003e\\)，其中：\n\\(\\mathcal{A}\\) 为动作集合 \\(\\mathcal{R}\\) 为奖励分布 对于每一个动作 \\(a\\)，定义其期望奖励为 \\(\\text{E}_{r \\sim \\mathcal{R}(\\cdot | a)} [r]\\)。 最优期望奖励表示为 \\(\\mathcal{Q}^{*} = \\max_{a \\in \\mathcal{A}} \\mathcal{Q}(a)\\)，懊悔 \\(\\mathcal{R}(a) = \\mathcal{Q}^{*} - \\mathcal{Q}(a)\\)。\n#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy as np class BernoulliBandit: def __init__(self, K): \u0026#34;\u0026#34;\u0026#34;伯努利多臂老虎机, 输入 K 为拉杆个数\u0026#34;\u0026#34;\u0026#34; self.probs = np.random.uniform(size=K) # 随机生成K个0-1之间的数，作为每个拉杆的获奖概率 self.best_idx = np.argmax(self.probs) # 获奖概率最大的拉杆 self.best_prob = self.probs[self.best_idx] # 最大的获奖概率 self.K = K def step(self, k): # 当玩家选择了k号拉杆后，根据该老虎机k号拉杆获得奖励的概率返回 1（获奖）或 0（未获奖） if np.random.rand() \u0026lt; self.probs[k]: return 1 else: return 0 class Solver: def __init__(self, bandit: BernoulliBandit): \u0026#34;\u0026#34;\u0026#34;多臂老虎机算法基本框架\u0026#34;\u0026#34;\u0026#34; self.bandit = bandit self.counts = np.zeros(self.bandit.K) # 每个拉杆的尝试次数 self.regret = 0.0 # 当前步的累积懊悔 self.actions = list() # 维护一个列表，记录每一步的动作 self.regrets = list() # 维护一个列表，记录每一步的累积懊悔 def update_regret(self, k): # 计算累积懊悔并保存，k为本次行动选择的拉杆的编号 self.regret += self.bandit.best_prob - self.bandit.probs[k] self.regrets.append(self.regret) def run_one_step(self): # 返回当前行动选择哪一个拉杆，由每个具体的策略实现 raise NotImplementedError def run(self, num_steps): # 运行一定次数，num_steps为总运行次数 for _ in range(num_steps): k = self.run_one_step() self.counts[k] += 1 self.actions.append(k) self.update_regret(k) 1.2 上置信界算法 #\r不确定性度量 \\(U(a)\\)，它会随着一个动作被尝试次数的增加而减小。直观地说，UCB 算法在每次选择拉杆前，先估计拉动每根拉杆的期望奖励上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 \\(p\\) 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最优可能获得最大期望奖励的拉杆。\nclass UCB(Solver): def __init__(self, bandit, coef, init_prob=1.0): \u0026#34;\u0026#34;\u0026#34;UCB算法, 继承Solver类\u0026#34;\u0026#34;\u0026#34; super(UCB, self).__init__(bandit) self.total_count = 0 self.estimates = np.array([init_prob] * self.bandit.K) self.coef = coef def run_one_step(self): self.total_count += 1 ucb = self.estimates + self.coef * np.sqrt(np.log(self.total_count) / (2 * (self.counts + 1))) # 计算上置信界 k = np.argmax(ucb) # 选出上置信界最大的拉杆 r = self.bandit.step(k) self.estimates[k] += 1.0 / (self.counts[k] + 1) * (r - self.estimates[k]) return k 1.3 汤普森采样算法 #\r假设每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作 a 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。\nclass ThompsonSampling(Solver): def __init__(self, bandit): \u0026#34;\u0026#34;\u0026#34;汤普森采样算法, 继承Solver类\u0026#34;\u0026#34;\u0026#34; super(ThompsonSampling, self).__init__(bandit) self._a = np.ones(self.bandit.K) # 列表，表示每个拉杆奖励为1的次数 self._b = np.ones(self.bandit.K) # 列表，表示每个拉杆奖励为0的次数 def run_one_step(self): samples = np.random.beta(self._a, self._b) # 按照Beta分布采样一组奖励 k = np.argmax(samples) # 选出采样数值最大的拉杆 r = self.bandit.step(k) self._a[k] += r # 更新Beta分布的第一个参数 self._b[k] += 1 - r # 更新Beta分布的第二个参数 return k ","date":"2025-06-20","externalUrl":null,"permalink":"/posts/multi-armed-bandit/","section":"Posts","summary":"","title":"多臂老虎机问题","type":"posts"},{"content":"","date":"2025-06-19","externalUrl":null,"permalink":"/tags/paper/","section":"Tags","summary":"","title":"Paper","type":"tags"},{"content":" arXiv:2505.22653\rNoisy-Rewards-in-Learning-to-Reason\r主内容 #\r一些想法 #\r论文发现并证明了，基于模型奖励的强化学习方法在训练 LLM 时，“足够好”的奖励模型和“特别好”的奖励模型训练得到的 LLM 表现很接近，启示不必盲求奖励模型的高准确率。并且通过实验验证，无法获得“足够好”的奖励模型时，设计思考过程方法对奖励模型进行校准也能得到很好的提升，并且这种校准对高准确率的奖励模型也有效。\n论文设计的 Reasoning Pattern Reward（RPR）是对模型思考输出中关键词（如 First、Second、Finally）出现次数进行奖励，但对于更多的下游任务（分类、推荐、检索），应该怎样设计 RPR？而且论文中也提到随着训练的进行会引起模型的「过度思考」，承认仅使用 RPR 而不使用其他答案校验奖励可能会被模型「hack」并产生问题。所以强化学习领域有没有更合适的、对思考过程进行奖励的方法设计值得研究。\n","date":"2025-06-19","externalUrl":null,"permalink":"/posts/the-climb-carvers-wisdom-deeper-than-the-summit/","section":"Posts","summary":"","title":"The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason","type":"posts"},{"content":"","date":"2025-06-19","externalUrl":null,"permalink":"/tags/%E8%BF%87%E7%A8%8B%E5%A5%96%E5%8A%B1/","section":"Tags","summary":"","title":"过程奖励","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"}]